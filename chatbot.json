{
  "greetings": {
    "hello": "Hi there! üëã How can I help you today?",
    "hi": "Hello! üòä Want to dive into coding or data science?",
    "hey": "Hey! Ready to explore some code together?",
    "welcome": "Welcome! üéâ Glad to have you here ‚Äî let‚Äôs get started with your learning journey."
  },  
  "what_are_you_doing": {
    "working": "I'm currently processing data and refining insights to help you with coding or analytics tasks. üöÄ",
    "learning": "I'm diving into new concepts in web development and data science, expanding my knowledge base. üìö",
    "exploring": "I'm exploring creative ways to explain complex topics, from responsive design to machine learning models. üîç"
  },
  "love": {
    "Whom does the owner love?": "The owner loves Prachi, the dawn that brightens his every day"
  },
  "about":{
    "Who are you?":"I'm an AI-powered chatbot designed to provide in-depth explanations and insights on web development and data science topics. My goal is to help you understand complex concepts with clarity and practical examples.",
    "What is your purpose?":"My purpose is to assist learners by providing comprehensive explanations, code examples, and insights into web development (HTML, CSS, JavaScript) and data science (Python, NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn). I aim to transform brief definitions into detailed learning experiences.",
    "who designed you?":"I was designed by Shubham Kadam, a passionate developer and educator dedicated to making complex coding concepts accessible and engaging for learners at all levels.",
    "who created you?":"I was created by Shubham Kadam, a passionate developer and educator dedicated to making complex coding concepts accessible and engaging for learners at all levels."
  },
  "help": {
    "help": "I cover foundational web development (HTML, CSS, JavaScript) and the core Python ecosystem (Python language fundamentals, Data Science, NumPy, Pandas, Matplotlib, Seaborn, and Scikit-learn). Try asking a specific question like 'Explain what a Pandas DataFrame is' or 'How does cross-validation work?'",
    "what can you do": "I can provide detailed, expanded explanations for core concepts across web development and data science, including code examples and the 'why' behind each tool or technique. My goal is to transform one-line definitions into comprehensive learning blocks."
  },
  "html": {
    "html": "HyperText Markup Language (HTML) is the essential building block of the web. It's used to structure content using elements enclosed in **tags** (e.g., `<div>`, `<h1>`, `<p>`). HTML tells the browser what each piece of content represents: is it a heading, a paragraph, a link, or an image? It acts as the semantic backbone of any webpage.",
    "html tags": "HTML elements are defined by tags. Common tags include `<h1>` through `<h6>` for hierarchical headings, `<p>` for standard paragraphs of text, `<a>` for hyperlinks (the 'H' in HTML), `<img>` for embedding images, and structural tags like `<div>` and `<span>` for grouping content and applying styling."
  },
  "css": {
    "css": "Cascading Style Sheets (CSS) is the language used for describing the presentation of a document written in HTML. CSS is responsible for the visual aesthetics‚Äîcontrolling the **layout**, **colors**, **fonts**, **spacing**, and overall responsiveness of a website. It separates the document's presentation from its content.",
    "css flexbox": "CSS Flexbox (Flexible Box Layout) is a one-dimensional layout model designed to help distribute space and align items within a container, even when the items' sizes are unknown or dynamic. It makes complex layouts, like centering elements or building navigation bars, much simpler and more predictable using properties like `display: flex`, `justify-content`, and `align-items`."
  },
  "javascript": {
    "javascript": "JavaScript (JS) is a high-level, interpreted programming language that is one of the core technologies of the World Wide Web. It enables **interactive** and dynamic features on websites, such as handling user input, manipulating the Document Object Model (DOM), and managing asynchronous operations like network requests. It's the engine that brings web applications to life.",
    "js functions": "Functions in JavaScript are fundamental building blocks. They are **reusable blocks of code** that perform a specific task or calculate a value. They can accept inputs (parameters) and can return an output. Defining functions with the `function` keyword or as **arrow functions** (`=>`) helps modularize code and prevents repetition."
  },  
  "python": {
    "python": "Python is a powerful, high-level, interpreted programming language known for its **readability and simple syntax**. It has enormous versatility, making it a favorite for web development (Django/Flask), **data science**, machine learning, automation, and scripting. Its large standard library and community make it suitable for almost any task.",
    "python variables": "Variables in Python are used to **store data values** (e.g., numbers, text, complex objects). They are created simply by assigning a value using the assignment operator (`=`), without needing prior declaration or type specification. Python is dynamically typed, meaning the type is inferred at runtime.",
    "python data types": "Python has several built-in core data types: **integers** (`int`), floating-point numbers (`float`), and **strings** (`str`) are primitives. Complex types include **lists** (ordered, mutable sequences), **tuples** (ordered, immutable sequences), **dictionaries** (unordered collections of key-value pairs), and **booleans** (`bool`, representing True or False).",
    "python loops": "Loops are control flow statements used to repeatedly execute a block of code. The **`for` loop** is typically used for iterating over a sequence (like a list or string), while the **`while` loop** repeats as long as a specified condition is true. They are essential for processing collections of data efficiently.",
    "python conditional statements": "Conditional statements (`if`, `elif`, `else`) are used to execute different blocks of code based on whether a given condition evaluates to `True` or `False`. **`if`** starts the block, **`elif`** (else if) checks additional conditions sequentially, and **`else`** executes if all preceding conditions are false. Python relies heavily on **indentation** to define the scope of these blocks.",
    "python functions": "Python functions are defined using the **`def` keyword** followed by the function name and parentheses. They allow you to bundle a set of instructions into a reusable unit, promoting modularity and reducing code duplication. Functions can take arguments and optionally return a value using the `return` statement.",
    "python if": "Conditional statements (`if`, `elif`, `else`) are used to execute different blocks of code based on whether a given condition is true or false. **`if`** starts the block, **`elif`** (else if) checks additional conditions sequentially, and **`else`** executes if all preceding conditions are false. Python relies on **indentation** to define the scope of these blocks.",
    "python list": "A **list** is a fundamental, versatile data structure in Python. It is an **ordered and mutable** (changeable) collection of items, allowing you to store elements of different data types (e.g., integers, strings, objects). Lists are defined using square brackets (`[]`) and support operations like indexing, slicing, appending, and sorting.",
    "python dictionary": "A **dictionary** is an unordered collection of data values that are stored as **key-value pairs**. It is mutable and highly optimized for retrieving values when the key is known. Keys must be unique and immutable (like strings or tuples), while values can be any data type. Dictionaries are defined using curly braces (`{}`) like `{'name': 'Alice', 'age': 30}`.",
    "python decorators": "Decorators are a powerful advanced feature that allows you to **modify or wrap a function or method's behavior** without permanently changing its code. They are syntactic sugar, using the `@decorator_name` syntax placed immediately before a function definition, and are commonly used for logging, authorization, memoization, and timing execution.",
    "python generators": "Generators are a simple and memory-efficient way to create **iterators** in Python. They are defined like functions but use the **`yield` keyword** instead of `return`. The key difference is that a generator does not compute and store all its values in memory at once; it generates values one at a time, only when requested, making them ideal for processing large streams of data.",
    "python exception handling": "Exception handling is used to gracefully manage errors that occur during program execution. The **`try-except-finally`** block structure allows you to 'try' a block of code, 'except' specific errors if they occur, and execute the 'finally' block unconditionally. This prevents the program from crashing and allows for smooth error logging or recovery.",
    "python for loop": "The **`for` loop** is Python's primary mechanism for iteration, used to loop over (or iterate through) a sequence (like a list, tuple, dictionary, set, or string) or any other iterable object. It simplifies tasks like processing every item in a collection or running a fixed number of times using the `range()` function. The loop executes the indented code block once for each item in the sequence.",
    "python while loop": "The **`while` loop** is used to repeatedly execute a block of code as long as a specified Boolean condition remains `True`. It requires careful handling to ensure the condition eventually becomes `False` to prevent an **infinite loop**. It's typically used when the number of iterations is not known beforehand, such as reading data until the end of a file or waiting for a user action."
  },
  "data science": {
    "data science": "Data science is an interdisciplinary field that uses **scientific methods, processes, algorithms, and systems** to extract knowledge and insights from structured and unstructured data. It combines expertise in statistics, computer science, and domain knowledge to solve complex problems, make predictions, and drive evidence-based decision-making.",
    "data wrangling": "Also known as **data cleaning or preprocessing**, data wrangling is the crucial process of transforming raw data into a more usable and appropriate format for analysis. This involves handling missing values, dealing with outliers, correcting structural errors, merging datasets, and ensuring data consistency and quality.",
    "data visualization": "Data visualization involves the graphical representation of data and information using charts, plots, and interactive dashboards. Its main purpose is to **communicate complex data patterns, trends, and outliers clearly and efficiently**, allowing analysts and stakeholders to quickly gain insights that would be difficult to uncover from raw numbers alone.",
    "data analysis": "Data analysis is the systematic process of **examining, cleansing, transforming, and modeling data** to discover useful information, draw conclusions, and support decision-making. It involves exploratory techniques (like calculating descriptive statistics) and confirmatory techniques (like hypothesis testing) to understand the data's characteristics and relationships.",
    "machine learning": "Machine learning (ML) is a subset of artificial intelligence (AI) where systems learn from data, **identify patterns**, and make decisions or predictions with minimal human intervention. Instead of being explicitly programmed with rules, ML models use algorithms to iteratively improve performance on a specific task by training on large datasets.",
    "feature engineering": "Feature engineering is the process of **creating new input features** from existing raw data to help a machine learning model perform better. This often involves transforming variables (e.g., taking the log of a skewed feature), creating interaction terms, or extracting temporal information (e.g., 'day of week' from a timestamp). It is often the most critical step for model performance."
  },
  "numpy": {
    "numpy": "NumPy (Numerical Python) is the foundational library for scientific computing in Python. It provides powerful **N-dimensional array objects (ndarrays)** and an extensive collection of high-level mathematical functions to operate on these arrays, making array-based computations significantly faster and more memory-efficient than standard Python lists.",
    "numpy array": "A NumPy array is a grid of values, all of the **same type**, indexed by a tuple of non-negative integers. Unlike Python lists, NumPy arrays are designed to support large-scale numerical operations and are stored in a contiguous block of memory, which is essential for performance in scientific applications. You create them using `np.array()`.",
    "numpy reshape": "The `reshape()` function is used to **change the shape** (dimensions) of a NumPy array without changing the actual data contained within it. For example, reshaping a 1D array of 6 elements into a 2D array with 2 rows and 3 columns (`.reshape(2, 3)`) allows you to organize and process the data dimensionally for modeling or visualization.",
    "numpy mean": "The `np.mean()` function calculates the **arithmetic mean (average)** of the elements in a NumPy array. This function is a common initial step in statistical analysis and can be applied across the entire array or along a specified axis (row-wise or column-wise) in multi-dimensional arrays.",
    "numpy random": "The `numpy.random` module is used for generating various kinds of **pseudorandom numbers** and sampling from different probability distributions. This is critical for tasks like simulating data, initializing weights in neural networks, or randomly splitting datasets during model training.",
    "numpy broadcasting": "Broadcasting is a powerful NumPy mechanism that allows arrays with **different shapes** to be used in arithmetic operations. Subject to certain constraints (e.g., dimensions must either be equal or one of them must be 1), the smaller array is 'stretched' across the larger array so that they have compatible shapes for the operation to proceed efficiently."
  },
  "pandas": {
    "pandas": "Pandas is an essential Python library for **data manipulation, cleaning, and analysis**. It introduces two primary data structures: the **Series** (a one-dimensional labeled array) and the **DataFrame** (a two-dimensional, tabular, labeled data structure, similar to a spreadsheet or SQL table). It provides high-performance tools for working with structured data.",
    "pandas dataframe": "A **DataFrame** is the core Pandas data structure, representing data in a **tabular format** with rows and columns. It is mutable and consists of three components: the data, the row index, and the column index (names). It allows for powerful operations like filtering rows, adding columns, and aggregating data based on labels.",
    "pandas read csv": "The `pd.read_csv()` function is the standard and most common way to **load data from a CSV (Comma Separated Values) file** into a Pandas DataFrame. It intelligently handles various delimiters, encoding formats, and data types, providing a seamless way to ingest external structured data for immediate analysis.",
    "pandas head": "The `df.head()` method is used to quickly **view the first 5 rows** (by default) of a DataFrame. This is an indispensable exploratory tool used immediately after loading data to confirm that the data was loaded correctly, to inspect the column names, and to quickly check the data types and content of the initial records.",
    "pandas describe": "The `df.describe()` method generates a **summary of descriptive statistics** of the numerical columns in a DataFrame. This output includes the count, mean, standard deviation, minimum, maximum, and the quartile values (25th, 50th, 75th percentiles), providing a rapid overview of the data's central tendency and dispersion.",
    "pandas groupby": "The `df.groupby()` method is used to **group rows of data together** based on the values in one or more columns. Once grouped, you can apply an aggregate function (like `mean()`, `sum()`, `count()`, or `max()`) to calculate group-specific statistics. This is central to performing segment-based analysis in data science.",
    "pandas merge": "The `pd.merge()` function is used to **combine DataFrames** based on common column values, similar to SQL joins. You can perform inner, outer, left, or right joins to combine data from different sources into a unified structure for analysis, making data integration seamless and flexible.",
    "pandas apply": "The `df.apply()` method is a powerful tool used to **apply a custom function** along either the row or column axis of a DataFrame. It is frequently used for performing complex, element-wise or series-wise transformations that are not easily achieved with built-in Pandas functions, often using Python's `lambda` functions for concise operations."
  },
  "matplotlib": {
    "matplotlib": "Matplotlib is a comprehensive, low-level Python library for creating **static, animated, and interactive visualizations** in Python. It provides a flexible, MATLAB-like plotting framework, making it the bedrock upon which many other data visualization libraries are built. It offers fine-grained control over every element of a plot.",
    "matplotlib plot": "The `plt.plot(x, y)` function is the most basic command in Matplotlib, used to create a **line plot** by connecting a series of (x, y) coordinates with lines. It is primarily used for visualizing time series data, trends, or any data where the order of points is meaningful.",
    "matplotlib bar": "The `plt.bar(categories, values)` function is used to create a **bar chart**, which represents categorical data with rectangular bars whose lengths are proportional to the values they represent. Bar charts are excellent for comparing different, discrete categories.",
    "matplotlib hist": "The `plt.hist(data)` function is used to create a **histogram**, which visualizes the distribution of a single numerical variable. The data is divided into bins (ranges), and the height of each bar represents the frequency (or count) of data points that fall into that specific bin, showing the shape of the data's distribution.",
    "matplotlib show": "The `plt.show()` function is essential in many environments (like Python scripts or older notebooks) to **display the current figure and all pending plots**. Without this call, the plot might not be rendered or updated in the output environment.",
    "matplotlib subplots": "The `plt.subplots()` function is the recommended way to create a figure and a set of subplots (axes) simultaneously. This function returns both the **Figure object** (the overall window/canvas) and an **Axes object** (the individual plot area), which allows for cleaner object-oriented plotting where you explicitly command each axis."
  },
  "seaborn": {
    "seaborn": "Seaborn is a high-level, statistical data visualization library built on top of Matplotlib. It provides a more user-friendly interface and highly attractive, informative statistical graphics, making it easy to create complex visualizations like **heatmaps, violin plots, and multi-panel charts** with minimal code.",
    "seaborn scatter": "The `sns.scatterplot()` function creates a **scatter plot**, which uses Cartesian coordinates to display values for two variables in a dataset. Each row in the data is represented by a marker, allowing you to visualize the relationship, correlation, or clustering between those two variables.",
    "seaborn heatmap": "The `sns.heatmap(df.corr())` function is typically used to visualize a **correlation matrix**. It displays the strength and direction of the relationships between pairs of variables in a dataset using color intensity, providing a fast visual check for multicollinearity or related features.",
    "seaborn boxplot": "The `sns.boxplot()` function creates a **box plot** (or box-and-whisker plot), which is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum. It's excellent for identifying outliers and comparing distributions across different categories.",
    "seaborn pairplot": "The `sns.pairplot(df)` function creates a matrix of scatter plots for all numerical variables in a DataFrame, where the diagonal plots show the distribution (often a histogram or KDE) of each variable. It is used for **high-level exploratory data analysis** to quickly visualize all pairwise relationships in a dataset.",
    "seaborn displot": "The `sns.displot()` function provides a flexible interface for plotting the **distribution of a single variable**. It can draw histograms, Kernel Density Estimates (KDEs), and empirical cumulative distribution functions, and it includes robust options for splitting the plot across rows and columns based on categorical variables (faceting)."
  },
  "machine learning": {
    "machine learning": "Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to **perform a specific task without using explicit instructions**, relying on patterns and inference instead. The process involves training a model on data and then using it to make predictions or decisions.",
    "supervised learning": "Supervised learning is a machine learning paradigm where the algorithm is trained on a **labeled dataset** (input data paired with the correct output, or 'label'). The model learns a function that maps the input to the output. The two main types are **Classification** (predicting discrete labels) and **Regression** (predicting continuous values).",
    "unsupervised learning": "Unsupervised learning is used when the training data is **unlabeled**, meaning the model must infer hidden patterns or structures on its own. The primary goals are to **explore the data** and find meaningful groupings (Clustering) or to reduce the number of variables (Dimensionality Reduction) for simplification.",
    "classification": "Classification is a supervised learning task where the model predicts a **discrete class label** or category for a given input. Examples include identifying an email as 'spam' or 'not spam' (binary) or identifying an image as 'cat,' 'dog,' or 'bird' (multi-class). Key algorithms include Logistic Regression and Support Vector Machines.",
    "regression": "Regression is a supervised learning task where the model predicts a **continuous numerical value** based on input variables. Examples include predicting house prices, stock market values, or the temperature tomorrow. Linear Regression is the most common starting point for this type of problem.",
    "clustering": "Clustering is an unsupervised learning task that aims to **group a set of objects** in such a way that objects in the same group (cluster) are more similar to each other than to those in other groups. Algorithms like **K-Means** or DBSCAN are used to discover intrinsic groupings in the data.",
    "model training": "Model training is the phase where the machine learning algorithm is **exposed to the training data** and iteratively adjusts its internal parameters to minimize a cost function (error). In libraries like Scikit-learn, this is typically done using the `model.fit(X_train, y_train)` method, where `X` is the features and `y` is the targets.",
    "model prediction": "Model prediction is the phase where the **trained model is used to forecast** an output for new, unseen input data. This is typically implemented with the `model.predict(X_test)` method in Scikit-learn, which takes a set of features (`X_test`) and returns the model's best guess for the corresponding target values.",
    "model evaluation": "Model evaluation is the process of **assessing the performance** and reliability of a trained machine learning model. For classification, metrics include **Accuracy, Precision, Recall, and F1-Score**. For regression, metrics like Mean Squared Error (MSE) and R-squared are used to quantify how well the model generalizes to new data.",
    "cross validation": "Cross-validation is a robust resampling technique used to **evaluate a model's generalization ability** and prevent overfitting. The most common method, K-Fold, involves splitting the training data into K subsets. The model is trained K times, using K-1 folds for training and the remaining fold for validation, and the results are averaged for a more reliable performance score.",
    "train test split": "The **train-test split** is a mandatory initial step in model development where the dataset is partitioned into two mutually exclusive subsets: the **training set** (used to fit the model) and the **test set** (used to evaluate its final performance). A common split is 80% for training and 20% for testing to ensure the model's performance is assessed on completely unseen data.",
    "feature scaling": "Feature scaling is a data preprocessing technique used to **standardize the independent variables** (features) in a dataset into a specific range. Algorithms sensitive to scale (like K-Nearest Neighbors or Support Vector Machines) perform better when features are on a comparable scale. Common methods are **Standardization** (Z-score) and **Normalization** (Min-Max scaling).",
    "pipeline": "A **Pipeline** in Scikit-learn is a powerful utility that allows you to chain multiple data preprocessing and modeling steps into a single, cohesive workflow. This simplifies the code, ensures the correct transformations are applied to the data in the correct order, and prevents **data leakage** (a critical concept where test set information unintentionally contaminates the training phase).",
    "hyperparameter tuning": "Hyperparameter tuning is the process of **finding the optimal set of parameters** (settings that are external to the model and cannot be learned from data, e.g., the 'k' in K-Nearest Neighbors or the 'depth' of a Decision Tree) for a given machine learning model. Techniques like **Grid Search** and **Randomized Search** are used to systematically explore the hyperparameter space to maximize model performance."
  },
  "scikit-learn": {
    "scikit-learn": "Scikit-learn is the **most popular and widely used Python library for classical machine learning**. It provides simple and efficient tools for data mining and data analysis, covering classification, regression, clustering, dimensionality reduction, model selection, and preprocessing. It is known for its consistent API across all algorithms.",
    "sklearn linear regression": "The `LinearRegression()` model fits a linear equation to the observed data to **model the relationship between a single dependent variable and one or more independent variables**. It is the simplest and most interpretable regression algorithm, aiming to minimize the sum of the squared differences between the predicted and actual continuous values.",
    "sklearn logistic regression": "Despite its name, `LogisticRegression()` is a robust and highly effective algorithm used for **binary and multi-class classification**. It estimates the probability that an input belongs to a certain class by using a logistic function (or sigmoid) to map the linear combination of input features to a probability value between 0 and 1.",
    "sklearn decision tree": "A `DecisionTreeClassifier()` is a non-parametric supervised learning method used for both classification and regression. It builds a model in the form of a **tree structure**, where internal nodes represent feature splits (tests on an attribute), branches represent the outcome of the test, and leaf nodes represent the class label (decision).",
    "sklearn random forest": "The `RandomForestClassifier()` is an **ensemble learning method** for classification and regression that operates by constructing a large number of decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is highly effective at reducing overfitting and improving accuracy.",
    "sklearn knn": "The `KNeighborsClassifier()` (K-Nearest Neighbors) is a simple, non-parametric, **instance-based learning algorithm** used for classification. An input is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its K nearest neighbors (where K is a user-defined integer).",
    "sklearn svm": "A `SVC()` (Support Vector Classifier) or **Support Vector Machine (SVM)** is a powerful and versatile model used for classification and regression. It works by finding the **optimal hyper-plane** that distinctly separates data points into classes in a high-dimensional space, maximizing the margin between the classes.",
    "sklearn kmeans": "The `KMeans()` algorithm is an **unsupervised clustering method** that partitions $n$ observations into $k$ clusters. It works by iteratively assigning data points to the nearest cluster center (centroid) and then recalculating the centroid based on the newly assigned points, aiming to minimize the within-cluster variance.",
    "sklearn train test split": "The `train_test_split()` function from `sklearn.model_selection` is the standard tool to **randomly divide a dataset** into separate training and testing subsets. It is crucial for ensuring that the model is evaluated on data it has never seen before, preventing an overly optimistic estimate of performance.",
    "sklearn accuracy": "The `accuracy_score(y_true, y_pred)` metric calculates the **ratio of correctly predicted instances** to the total number of instances. While intuitive, it can be misleading in imbalanced datasets, but remains a common baseline metric for classification tasks, representing overall correctness.",
    "sklearn confusion matrix": "A `confusion_matrix()` is a specific table layout that allows **visualization of the performance of a classification algorithm**. It clearly shows the number of **True Positives, True Negatives, False Positives (Type I error), and False Negatives (Type II error)**, which are the building blocks for calculating more advanced metrics like precision and recall.",
    "sklearn classification report": "The `classification_report()` function generates a text report showing the main classification metrics (Precision, Recall, F1-Score, and Support) **per class**. This detailed, per-class breakdown is essential for understanding where a model performs well and where it struggles, especially in multi-class problems.",
    "sklearn cross validation": "The `cross_val_score()` function is used to perform cross-validation (typically K-Fold) on a specified model and dataset. It returns an array of scores, one for each fold, which are then typically averaged to provide a much **more stable and reliable estimate of the model's true performance** than a single train-test split.",
    "sklearn pipeline": "The `Pipeline()` utility is fundamental for combining transformers (like `StandardScaler` or `OneHotEncoder`) and a final estimator (like `LogisticRegression`) into a single object. This streamlined workflow ensures that the entire process, from preprocessing to prediction, is **applied consistently and correctly** across training and prediction phases.",
    "sklearn grid search": "The `GridSearchCV()` function from `sklearn.model_selection` is a technique for **hyperparameter tuning**. It systematically works through multiple combinations of parameter values, trains a model for each combination using cross-validation, and returns the model with the best performance score based on the specified evaluation metric."
  },
  "Good Bye":{
    "Bye": "Goodbye! If you have more questions about web development or data science, feel free to ask anytime.",
    "Good bye": "Goodbye! happy coding!",
    "Lets meet again": "Sure! I'm always here to help you with web development and data science topics. Just reach out whenever you need assistance."
  },
  "default": "Sorry, I didn‚Äôt get that. Ask me about HTML, CSS, JavaScript, Python, or Data Science topics."
}